import feedparser
import datetime
import requests
from urllib.parse import parse_qs, urlparse
import re

class YouTubeRSSReader:
    def __init__(self):
        self.channels = []
        self.new_videos = []
        self.skip_shorts = True
        self.cutoff_hours = 36
    
    def set_skip_shorts(self, skip=True):
        """Thi·∫øt l·∫≠p c√≥ b·ªè qua Shorts hay kh√¥ng"""
        self.skip_shorts = skip
    
    def set_cutoff_hours(self, hours=36):
        """Thi·∫øt l·∫≠p th·ªùi gian qu√©t video (m·∫∑c ƒë·ªãnh 36 gi·ªù)"""
        self.cutoff_hours = hours
    
    def add_channel(self, channel_id=None, channel_url=None, channel_name=None):
        """Th√™m k√™nh YouTube ƒë·ªÉ theo d√µi"""
        if channel_url and not channel_id:
            # Tr√≠ch xu·∫•t channel ID t·ª´ URL
            if '/channel/' in channel_url:
                channel_id = channel_url.split('/channel/')[1].split('/')[0]
            elif '/c/' in channel_url or '/@' in channel_url:
                print(f"‚ö†Ô∏è C·∫ßn channel ID cho {channel_url}. Vui l√≤ng s·ª≠ d·ª•ng URL d·∫°ng /channel/")
                return False
        
        if channel_id:
            rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
            channel_main_url = f"https://www.youtube.com/channel/{channel_id}"
            
            self.channels.append({
                'id': channel_id,
                'name': channel_name or channel_id,
                'rss_url': rss_url,
                'channel_url': channel_main_url
            })
            print(f"‚úÖ ƒê√£ th√™m k√™nh: {channel_name or channel_id}")
            return True
        
        print(f"‚ùå Kh√¥ng th·ªÉ th√™m k√™nh: thi·∫øu channel_id")
        return False
    
    def add_channels_from_list(self, channels_list):
        """Th√™m nhi·ªÅu k√™nh t·ª´ danh s√°ch"""
        for channel_info in channels_list:
            if isinstance(channel_info, tuple) and len(channel_info) == 2:
                channel_id, channel_name = channel_info
                self.add_channel(channel_id=channel_id, channel_name=channel_name)
            elif isinstance(channel_info, dict):
                self.add_channel(**channel_info)
    
    def is_youtube_short(self, entry):
        """Ki·ªÉm tra xem video c√≥ ph·∫£i YouTube Shorts kh√¥ng"""
        # Ki·ªÉm tra URL
        if '/shorts/' in entry.link:
            return True
        
        # Ki·ªÉm tra title
        title = entry.title.lower()
        if title.startswith('#') or '#shorts' in title or '#short' in title:
            return True
        
        # Ki·ªÉm tra description
        description = entry.get('summary', '').lower()
        if '#shorts' in description or '#short' in description:
            return True
        
        return False
    
    def normalize_youtube_url(self, url):
        """Chu·∫©n h√≥a URL YouTube"""
        if not url:
            return url
        
        # Tr√≠ch xu·∫•t video ID
        patterns = [
            r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})',
            r'youtube\.com\/.*[?&]v=([a-zA-Z0-9_-]{11})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                video_id = match.group(1)
                return f"https://www.youtube.com/watch?v={video_id}"
        
        return url
    
    def fetch_recent_videos(self, hours=None):
        """L·∫•y video m·ªõi trong kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh"""
        if hours is None:
            hours = self.cutoff_hours
            
        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)
        self.new_videos = []
        
        print(f"üîç Qu√©t video m·ªõi trong {hours} gi·ªù qua (sau {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')})")
        
        for channel in self.channels:
            print(f"\nüì∫ ƒêang ki·ªÉm tra k√™nh: {channel['name']}")
            
            try:
                # L·∫•y RSS feed
                feed = feedparser.parse(channel['rss_url'])
                
                if feed.bozo:
                    print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc RSS feed cho {channel['name']}: {feed.bozo_exception}")
                    continue
                
                channel_videos = 0
                
                # Ki·ªÉm tra t·ª´ng video
                for entry in feed.entries:
                    # Chuy·ªÉn ƒë·ªïi th·ªùi gian published
                    published_time = datetime.datetime(*entry.published_parsed[:6])
                    
                    # Ch·ªâ l·∫•y video m·ªõi trong kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh
                    if published_time > cutoff_time:
                        # Ki·ªÉm tra xem c√≥ ph·∫£i YouTube Shorts kh√¥ng
                        is_short = self.is_youtube_short(entry)
                        
                        if not (self.skip_shorts and is_short):
                            # Chu·∫©n h√≥a URL
                            normalized_url = self.normalize_youtube_url(entry.link)
                            
                            # Tr√≠ch xu·∫•t video ID
                            video_id = ''
                            if 'v=' in entry.link:
                                video_id = entry.link.split('v=')[1].split('&')[0]
                            
                            video_info = {
                                'title': entry.title,
                                'url': normalized_url,
                                'original_url': entry.link,
                                'channel': channel['name'],
                                'channel_url': channel['channel_url'],
                                'upload_date': published_time.strftime('%Y-%m-%d'),
                                'published_datetime': published_time,
                                'description': entry.get('summary', ''),
                                'video_id': video_id,
                                'is_short': is_short
                            }
                            
                            self.new_videos.append(video_info)
                            channel_videos += 1
                            
                            video_type = "Shorts" if is_short else "Video"
                            print(f"   ‚úÖ {video_type}: {entry.title}")
                        else:
                            print(f"   ‚è≠Ô∏è B·ªè qua Shorts: {entry.title}")
                
                print(f"   üìä T√¨m th·∫•y {channel_videos} video m·ªõi t·ª´ {channel['name']}")
                
            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω k√™nh {channel['name']}: {str(e)}")
        
        # S·∫Øp x·∫øp video theo th·ªùi gian m·ªõi nh·∫•t
        self.new_videos.sort(key=lambda x: x['published_datetime'], reverse=True)
        
        print(f"\nüéØ T·ªïng c·ªông t√¨m th·∫•y {len(self.new_videos)} video m·ªõi")
        return self.new_videos
    
    def get_video_list_for_processing(self):
        """Tr·∫£ v·ªÅ danh s√°ch video theo format m√† script 1 c·∫ßn"""
        return self.new_videos
    
    def print_summary(self):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        if not self.new_videos:
            print("\n‚ùå Kh√¥ng c√≥ video m·ªõi n√†o.")
            return
        
        print(f"\nüìã T·ªîNG K·∫æT:")
        print(f"   üì∫ T·ªïng s·ªë video: {len(self.new_videos)}")
        
        # Th·ªëng k√™ theo k√™nh
        channel_stats = {}
        for video in self.new_videos:
            channel = video['channel']
            if channel not in channel_stats:
                channel_stats[channel] = 0
            channel_stats[channel] += 1
        
        print(f"   üìä Ph√¢n b·ªë theo k√™nh:")
        for channel, count in sorted(channel_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"      ‚Ä¢ {channel}: {count} video")
        
        # Th·ªëng k√™ theo lo·∫°i
        shorts_count = sum(1 for v in self.new_videos if v.get('is_short', False))
        regular_count = len(self.new_videos) - shorts_count
        
        print(f"   üìπ Lo·∫°i video:")
        print(f"      ‚Ä¢ Video th∆∞·ªùng: {regular_count}")
        print(f"      ‚Ä¢ Shorts: {shorts_count}")

# H√†m ch√≠nh ƒë·ªÉ t√≠ch h·ª£p v√†o script 1
def get_latest_videos_from_rss(return_links=True, hours=36, skip_shorts=True):
    """
    H√†m ch√≠nh ƒë·ªÉ l·∫•y danh s√°ch video m·ªõi t·ª´ RSS feeds
    Thay th·∫ø cho get_latest_video2.main()
    """
    
    # Danh s√°ch c√°c k√™nh YouTube
    channels_to_monitor = [
        ("UCLXo7UDZvByw2ixzpQCufnA", "Vox"),
        ("UCvJJ_dzjViJCoLf5uKUTwoA", "CNBC"),
        ("UCHnyfMqiRRG1u-2MsSQLbXA", "Veritasium"),
        ("UCpVm7bg6pXKo1Pr6k5kxG9A", "NatGeo"),
        ("UCK7tptUDHh-RYDsdxO1-5QQ", "WSJ"),
        ("UCZYTClx2T1of7BRZ86-8fow", "SciShow"),
        ("UCcyq283he07B7_KUX07mmtA", "Business Insider"),
        ("UCwmZiChSryoWQCZMIQezgTg", "BBC Earth"),
        ("UCODHrzPMGbNv67e84WDZhQQ", "Fern"),
        ("UCsBjURrPoezykLs9EqgamOA", "Fireship"),
        ("UCtRFmSyL4fSLQkn-wMqlmdA", "History of the Universe"),
        ("UCKWaEZ-_VweaEx1j62do_vQ", "IBM Technology"),
        ("UCDPk9MG2RexnOMGTD-YnSnA", "Nat Geo Animals"),
        ("UCmGSJVG3mCRXVOP4yZrU1Dw", "Johnny Harris"),
        ("UC6ktP3PLU5sAJxN9Rb0TALg", "Mike Shake"),
        ("UCtYKe7-XbaDjpUwcU5x0bLg", "Neo"),
        ("UCoxcjq-8xIDTYp3uz647V5A", "Numberphile"),
        ("UC7_gcs09iThXybpVgjHZ_7g", "PBS Space Time"),
        ("UCQSpnDG3YsFNf5-qHocF-WQ", "ThioJoe"),
        ("UCsooa4yRKGN_zEE8iknghZA", "TED-Ed"),
        ("UCAuUUnT6oDeKwE6v1NGQxug", "TED"),
        ("UCEIwxahdLz7bap-VDs9h35A", "Steve Mould"),
        ("UC1yNl2E66ZzKApQdRuTQ4tw", "Sabine Hossenfelder"),
        ("UCTpmmkp1E4nmZqWPS-dl5bg", "Quanta Science"),
        ("UCgNg3vwj3xt7QOrcIDaHdFg", "PolyMatter"),
        ("UCMOqf8ab-42UUQIdVoKwjlQ", "Practical Engineering"),
        ("UC513PdAP2-jWkJunTh5kXRw", "CrunchLabs"),
        ("UCW39zufHfsuGgpLviKh297Q", "DW Documentary"),
        ("UCHaHD477h-FeBbVh9Sh7syA", "BBC Learning English"),
        ("UCvK4bOhULCpmLabd2pDMtnA", "Yes Theory"),
        ("UC9RM-iSvTu1uPJb8X5yp3EQ", "Wendover Productions"),
        ("UC4JX40jDee_tINbkjycV4Sg", "Tech With Tim"),
        ("UC6biysICWOJ-C3P4Tyeggzg", "Low Level"),
    ]
    
    # T·∫°o RSS reader
    reader = YouTubeRSSReader()
    reader.set_skip_shorts(skip_shorts)
    reader.set_cutoff_hours(hours)
    
    # Th√™m c√°c k√™nh
    reader.add_channels_from_list(channels_to_monitor)
    
    # L·∫•y video m·ªõi
    videos = reader.fetch_recent_videos(hours)
    
    # In t√≥m t·∫Øt
    reader.print_summary()
    
    if return_links:
        return videos
    else:
        return len(videos)

# H√†m ƒë·ªÉ test ƒë·ªôc l·∫≠p
def main():
    """H√†m test ch√≠nh"""
    print("üöÄ B·∫Øt ƒë·∫ßu qu√©t video m·ªõi t·ª´ RSS feeds...")
    
    videos = get_latest_videos_from_rss(
        return_links=True,
        hours=36,
        skip_shorts=True
    )
    
    if videos:
        print(f"\nüìù Danh s√°ch {len(videos)} video m·ªõi:")
        print("=" * 80)
        
        for i, video in enumerate(videos, 1):
            print(f"{i}. [{video['channel']}] {video['title']}")
            print(f"   üîó {video['url']}")
            print(f"   üìÖ {video['upload_date']}")
            print("-" * 80)
    else:
        print("\n‚ùå Kh√¥ng t√¨m th·∫•y video m·ªõi n√†o.")

if __name__ == "__main__":
    main()


# import feedparser
# import datetime
# import requests
# from urllib.parse import parse_qs, urlparse
# import re

# class YouTubeRSSReader:
#     def __init__(self):
#         self.channels = []
#         self.new_videos = []
#         self.skip_shorts = True
#         self.cutoff_hours = 36
    
#     def set_skip_shorts(self, skip=True):
#         """Thi·∫øt l·∫≠p c√≥ b·ªè qua Shorts hay kh√¥ng"""
#         self.skip_shorts = skip
    
#     def set_cutoff_hours(self, hours=36):
#         """Thi·∫øt l·∫≠p th·ªùi gian qu√©t video (m·∫∑c ƒë·ªãnh 36 gi·ªù)"""
#         self.cutoff_hours = hours
    
#     def add_channel(self, channel_id=None, channel_url=None, channel_name=None):
#         """Th√™m k√™nh YouTube ƒë·ªÉ theo d√µi"""
#         if channel_url and not channel_id:
#             # Tr√≠ch xu·∫•t channel ID t·ª´ URL
#             if '/channel/' in channel_url:
#                 channel_id = channel_url.split('/channel/')[1].split('/')[0]
#             elif '/c/' in channel_url or '/@' in channel_url:
#                 print(f"‚ö†Ô∏è C·∫ßn channel ID cho {channel_url}. Vui l√≤ng s·ª≠ d·ª•ng URL d·∫°ng /channel/")
#                 return False
        
#         if channel_id:
#             rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
#             channel_main_url = f"https://www.youtube.com/channel/{channel_id}"
            
#             self.channels.append({
#                 'id': channel_id,
#                 'name': channel_name or channel_id,
#                 'rss_url': rss_url,
#                 'channel_url': channel_main_url
#             })
#             print(f"‚úÖ ƒê√£ th√™m k√™nh: {channel_name or channel_id}")
#             return True
        
#         print(f"‚ùå Kh√¥ng th·ªÉ th√™m k√™nh: thi·∫øu channel_id")
#         return False
    
#     def add_channels_from_list(self, channels_list):
#         """Th√™m nhi·ªÅu k√™nh t·ª´ danh s√°ch"""
#         for channel_info in channels_list:
#             if isinstance(channel_info, tuple) and len(channel_info) == 2:
#                 channel_id, channel_name = channel_info
#                 self.add_channel(channel_id=channel_id, channel_name=channel_name)
#             elif isinstance(channel_info, dict):
#                 self.add_channel(**channel_info)
    
#     def is_youtube_short(self, entry):
#         """Ki·ªÉm tra xem video c√≥ ph·∫£i YouTube Shorts kh√¥ng"""
#         # Ki·ªÉm tra URL
#         if '/shorts/' in entry.link:
#             return True
        
#         # Ki·ªÉm tra title
#         title = entry.title.lower()
#         if title.startswith('#') or '#shorts' in title or '#short' in title:
#             return True
        
#         # Ki·ªÉm tra description
#         description = entry.get('summary', '').lower()
#         if '#shorts' in description or '#short' in description:
#             return True
        
#         return False
    
#     def normalize_youtube_url(self, url):
#         """Chu·∫©n h√≥a URL YouTube"""
#         if not url:
#             return url
        
#         # Tr√≠ch xu·∫•t video ID
#         patterns = [
#             r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})',
#             r'youtube\.com\/.*[?&]v=([a-zA-Z0-9_-]{11})',
#         ]
        
#         for pattern in patterns:
#             match = re.search(pattern, url)
#             if match:
#                 video_id = match.group(1)
#                 return f"https://www.youtube.com/watch?v={video_id}"
        
#         return url
    
#     def fetch_recent_videos(self, hours=None):
#         """L·∫•y video m·ªõi trong kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh"""
#         if hours is None:
#             hours = self.cutoff_hours
            
#         cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)
#         self.new_videos = []
        
#         print(f"üîç Qu√©t video m·ªõi trong {hours} gi·ªù qua (sau {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')})")
        
#         for channel in self.channels:
#             print(f"\nüì∫ ƒêang ki·ªÉm tra k√™nh: {channel['name']}")
            
#             try:
#                 # L·∫•y RSS feed
#                 feed = feedparser.parse(channel['rss_url'])
                
#                 if feed.bozo:
#                     print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc RSS feed cho {channel['name']}: {feed.bozo_exception}")
#                     continue
                
#                 channel_videos = 0
                
#                 # Ki·ªÉm tra t·ª´ng video
#                 for entry in feed.entries:
#                     # Chuy·ªÉn ƒë·ªïi th·ªùi gian published
#                     published_time = datetime.datetime(*entry.published_parsed[:6])
                    
#                     # Ch·ªâ l·∫•y video m·ªõi trong kho·∫£ng th·ªùi gian ch·ªâ ƒë·ªãnh
#                     if published_time > cutoff_time:
#                         # Ki·ªÉm tra xem c√≥ ph·∫£i YouTube Shorts kh√¥ng
#                         is_short = self.is_youtube_short(entry)
                        
#                         if not (self.skip_shorts and is_short):
#                             # Chu·∫©n h√≥a URL
#                             normalized_url = self.normalize_youtube_url(entry.link)
                            
#                             video_info = {
#                                 'title': entry.title,
#                                 'url': normalized_url,
#                                 'original_url': entry.link,
#                                 'channel': channel['name'],
#                                 'channel_url': channel['channel_url'],
#                                 'upload_date': published_time.strftime('%Y-%m-%d'),
#                                 'published_datetime': published_time,
#                                 'description': entry.get('summary', ''),
#                                 'video_id': entry.link.split('v=')[1].split('&')[0] if 'v=' in entry.link else '',
#                                 'is_short': is_short
#                             }
                            
#                             self.new_videos.append(video_info)
#                             channel_videos += 1
                            
#                             video_type = "Shorts" if is_short else "Video"
#                             print(f"   ‚úÖ {video_type}: {entry.title}")
#                         else:
#                             print(f"   ‚è≠Ô∏è B·ªè qua Shorts: {entry.title}")
                
#                 print(f"   üìä T√¨m th·∫•y {channel_videos} video m·ªõi t·ª´ {channel['name']}")
                
#             except Exception as e:
#                 print(f"‚ùå L·ªói khi x·ª≠ l√Ω k√™nh {channel['name']}: {str(e)}")
        
#         # S·∫Øp x·∫øp video theo th·ªùi gian m·ªõi nh·∫•t
#         self.new_videos.sort(key=lambda x: x['published_datetime'], reverse=True)
        
#         print(f"\nüéØ T·ªïng c·ªông t√¨m th·∫•y {len(self.new_videos)} video m·ªõi")
#         return self.new_videos
    
#     def get_video_list_for_processing(self):
#         """Tr·∫£ v·ªÅ danh s√°ch video theo format m√† script 1 c·∫ßn"""
#         return self.new_videos
    
#     def print_summary(self):
#         """In t√≥m t·∫Øt k·∫øt qu·∫£"""
#         if not self.new_videos:
#             print("\n‚ùå Kh√¥ng c√≥ video m·ªõi n√†o.")
#             return
        
#         print(f"\nüìã T·ªîNG K·∫æT:")
#         print(f"   üì∫ T·ªïng s·ªë video: {len(self.new_videos)}")
        
#         # Th·ªëng k√™ theo k√™nh
#         channel_stats = {}
#         for video in self.new_videos:
#             channel = video['channel']
#             if channel not in channel_stats:
#                 channel_stats[channel] = 0
#             channel_stats[channel] += 1
        
#         print(f"   üìä Ph√¢n b·ªë theo k√™nh:")
#         for channel, count in sorted(channel_stats.items(), key=lambda x: x[1], reverse=True):
#             print(f"      ‚Ä¢ {channel}: {count} video")
        
#         # Th·ªëng k√™ theo lo·∫°i
#         shorts_count = sum(1 for v in self.new_videos if v.get('is_short', False))
#         regular_count = len(self.new_videos) - shorts_count
        
#         print(f"   üìπ Lo·∫°i video:")
#         print(f"      ‚Ä¢ Video th∆∞·ªùng: {regular_count}")
#         print(f"      ‚Ä¢ Shorts: {shorts_count}")

# # H√†m ch√≠nh ƒë·ªÉ t√≠ch h·ª£p v√†o script 1
# def get_latest_videos_from_rss(return_links=True, hours=36, skip_shorts=True):
#     """
#     H√†m ch√≠nh ƒë·ªÉ l·∫•y danh s√°ch video m·ªõi t·ª´ RSS feeds
#     Thay th·∫ø cho get_latest_video2.main()
#     """
    
#     # Danh s√°ch c√°c k√™nh YouTube
#     channels_to_monitor = [
#         ("UCLXo7UDZvByw2ixzpQCufnA", "Vox"),
#         ("UCvJJ_dzjViJCoLf5uKUTwoA", "CNBC"),
#         ("UCHnyfMqiRRG1u-2MsSQLbXA", "Veritasium"),
#         ("UCpVm7bg6pXKo1Pr6k5kxG9A", "NatGeo"),
#         ("UCK7tptUDHh-RYDsdxO1-5QQ", "WSJ"),
#         ("UCZYTClx2T1of7BRZ86-8fow", "SciShow"),
#         ("UCcyq283he07B7_KUX07mmtA", "Business Insider"),
#         ("UCwmZiChSryoWQCZMIQezgTg", "BBC Earth"),
#         ("UCODHrzPMGbNv67e84WDZhQQ", "Fern"),
#         ("UCsBjURrPoezykLs9EqgamOA", "Fireship"),
#         ("UCtRFmSyL4fSLQkn-wMqlmdA", "History of the Universe"),
#         ("UCKWaEZ-_VweaEx1j62do_vQ", "IBM Technology"),
#         ("UCDPk9MG2RexnOMGTD-YnSnA", "Nat Geo Animals"),
#         ("UCmGSJVG3mCRXVOP4yZrU1Dw", "Johnny Harris"),
#         ("UC6ktP3PLU5sAJxN9Rb0TALg", "Mike Shake"),
#         ("UCtYKe7-XbaDjpUwcU5x0bLg", "Neo"),
#         ("UCoxcjq-8xIDTYp3uz647V5A", "Numberphile"),
#         ("UC7_gcs09iThXybpVgjHZ_7g", "PBS Space Time"),
#         ("UCQSpnDG3YsFNf5-qHocF-WQ", "ThioJoe"),
#         ("UCsooa4yRKGN_zEE8iknghZA", "TED-Ed"),
#         ("UCAuUUnT6oDeKwE6v1NGQxug", "TED"),
#         ("UCEIwxahdLz7bap-VDs9h35A", "Steve Mould"),
#         ("UC1yNl2E66ZzKApQdRuTQ4tw", "Sabine Hossenfelder"),
#         ("UCTpmmkp1E4nmZqWPS-dl5bg", "Quanta Science"),
#         ("UCgNg3vwj3xt7QOrcIDaHdFg", "PolyMatter"),
#         ("UCMOqf8ab-42UUQIdVoKwjlQ", "Practical Engineering"),
#         ("UC513PdAP2-jWkJunTh5kXRw", "CrunchLabs"),
#         ("UCW39zufHfsuGgpLviKh297Q", "DW Documentary"),
#         ("UCHaHD477h-FeBbVh9Sh7syA", "BBC Learning English"),
#         ("UCvK4bOhULCpmLabd2pDMtnA", "Yes Theory"),
#         ("UC9RM-iSvTu1uPJb8X5yp3EQ", "Wendover Productions"),
#     ]
    
#     # T·∫°o RSS reader
#     reader = YouTubeRSSReader()
#     reader.set_skip_shorts(skip_shorts)
#     reader.set_cutoff_hours(hours)
    
#     # Th√™m c√°c k√™nh
#     reader.add_channels_from_list(channels_to_monitor)
    
#     # L·∫•y video m·ªõi
#     videos = reader.fetch_recent_videos(hours)
    
#     # In t√≥m t·∫Øt
#     reader.print_summary()
    
#     if return_links:
#         return videos
#     else:
#         return len(videos)

# # H√†m ƒë·ªÉ test ƒë·ªôc l·∫≠p
# def main():
#     """H√†m test ch√≠nh"""
#     print("üöÄ B·∫Øt ƒë·∫ßu qu√©t video m·ªõi t·ª´ RSS feeds...")
    
#     videos = get_latest_videos_from_rss(
#         return_links=True,
#         hours=36,
#         skip_shorts=True
#     )
    
#     if videos:
#         print(f"\nüìù Danh s√°ch {len(videos)} video m·ªõi:")
#         print("=" * 80)
        
#         for i, video in enumerate(videos, 1):
#             print(f"{i}. [{video['channel']}] {video['title']}")
#             print(f"   üîó {video['url']}")
#             print(f"   üìÖ {video['upload_date']}")
#             print("-" * 80)
#     else:
#         print("\n‚ùå Kh√¥ng t√¨m th·∫•y video m·ªõi n√†o.")

# if __name__ == "__main__":
#     main()